from sgd import BasicStochasticGradientMethod
from loss import EmpiricalLossFn
import numpy as np
import sklearn.datasets


def basic_sgd_est():
    objective = lambda x: x*x
    gradient = lambda x: x*2
    sgd = BasicStochasticGradientMethod(34)
    gradients = [gradient for i in range(10000)]
    sgd.train(gradients)
    assert(np.isclose(sgd.w, 0, rtol=1e-6, atol=1e-6))

    sgd2 = BasicStochasticGradientMethod(34, lambda x: 0.1)
    gradients = [gradient for i in range(10000)]
    sgd2.train(gradients)
    assert(np.isclose(sgd2.w, 0, rtol=1e-6, atol=1e-6))
    assert(sgd2._train_iter < sgd._train_iter)


def sgd_changing_stepsize_est():
    objective = lambda x: x*x
    gradient = lambda x: x*2
    sgd = BasicStochasticGradientMethod(34, lambda x: 1.0/(x+1) )
    gradients = [gradient for i in range(10)]
    sgd.train(gradients)
    assert(np.isclose(sgd.w, 0, rtol=1e-6, atol=1e-6))

def hinge_loss_test():
    def objective(w, x):
        y = x[-1]
        x_ = x[:-1]
        return max(0, x_.dot(w)*y)

    def gradient_fn(w, x):
        y = x[-1]
        x_ = x[:-1]
        if (w.dot(x_) < 1):
            return -x_*y
        else:
            return np.zeros(x_.shape)

    hinge_loss = EmpiricalLossFn(objective, gradient_fn)
    digits = sklearn.datasets.load_digits(2)
    data = digits.data
    labels = (digits.target*2) - 1
    X = np.hstack((data,labels[np.newaxis].T))
    objective = hinge_loss.partial_eval_objective_full(X)
    gradients = [hinge_loss.partial_eval_gradient_streaming(x) for x in X]
    sgd = BasicStochasticGradientMethod(np.zeros(data[0].shape), lambda x: 0.01)
    for g in gradients:
        sgd.train_one(g)
        print "OBJECTIVE VALUE: {0}".format(objective(sgd.w))







